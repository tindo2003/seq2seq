{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42QVgkHVVk6z"
      },
      "source": [
        "# CIS 5300 Fall 2022 - Homework 4 | Semantic Parsing with Encoder-Decoder Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0E7AjHHPGHpn"
      },
      "source": [
        "#Introduction\n",
        "\n",
        "In this homework, you are going to implement a Seq2Seq model for semantic parsing. We will be converting natural language sentences into queries we can execute against a knowledgebase. To start you off, we've provided you code that set up your environment. First, run a nearest neighbor baseline that we provided to understand how the process looks like. Then you will need to implement your own Seq2Seq model and evalute on it.\n",
        "\n",
        "You should NOT change the class names, as parts of the code we are providing to simplify the developement depends on it. For each class, some initalization of parameters and the starter code are alreay provided, so you can simply fill in the code between `### BEGIN OF FILL-IN ###` and `### END OF FILL-IN ###` to complete this assignment. However, feel free to change the starter code and even the initalization of parameters if you prefer this way. In the end, you will only be evaluated on the quality of your outputs: as long as your model reaches the target performance levels, using Seq2Seq models, you will get full credit, regardless of implementation details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxbGqE4AQtkQ"
      },
      "source": [
        "# Preperation\n",
        "\n",
        "You should select the device (CPU vs GPU) in Colab. The whole homework is runable by CPU within a reasonable time (about 8 minutes), but GPU can speed up the model implementation more (about 2 minutes).\n",
        "\n",
        "Here, we provided the command line code to download the starter code from GitHub repo. You may also want to go through some other .py files in this project as some of the functions and classes are imported and used here, especially the *evaluate()* function from *lf_evaluator.py* and *Example()*, *Derivation()*, *load_datasets()* and some other functions from *data.py*. After you run the cell to clone the repo, you will find the files on the left-hand side in colab under Files, or https://github.com/realliyifei/cis5300-semantic-parsing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxOVg9gUrKx8"
      },
      "source": [
        "#Setting up\n",
        "\n",
        "The following few sections will help you set up java (needed for eval) and some other helper code and packages that may help you throughout this assignment. Please do not change them and run them first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIBy8NAw2Ym5",
        "outputId": "ea7aca29-fb29-4d6c-d71c-d7f5e67caf70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "openjdk version \"1.8.0_412\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_412-8u412-ga-1~22.04.1-b08)\n",
            "OpenJDK 64-Bit Server VM (build 25.412-b08, mixed mode)\n"
          ]
        }
      ],
      "source": [
        "#@title Set-up Java\n",
        "import os       #importing os to set environment variable\n",
        "def install_java():\n",
        "#   !apt update\n",
        "  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\n",
        "  !java -version       #check java version\n",
        "install_java()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d3UfyWuWyG2",
        "outputId": "e8a53d00-87d7-4089-9540-73c5a440288d"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/realliyifei/cis5300-semantic-parsing\n",
        "# %cd cis5300-semantic-parsing\n",
        "# !chmod a+x evaluator/geoquery # Solve permission issue on the `geoquery` file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "O3u4lYmAhQok"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import random\n",
        "import numpy as np\n",
        "from typing import List\n",
        "from tqdm.notebook import tqdm\n",
        "# import times\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable as Var\n",
        "from torch import optim\n",
        "\n",
        "# Import from starter code\n",
        "from utils import *\n",
        "from data import *\n",
        "from lf_evaluator import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WuVG_IsQ9DP"
      },
      "source": [
        "## Argument Parser\n",
        "\n",
        "Here we start you off with some common arguments that you may need for this homework. Feel free to extend it and include whatever arguments you feel needed for your model later!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_WKOSSyUn7aS"
      },
      "outputs": [],
      "source": [
        "def add_models_args(parser):\n",
        "    \"\"\"\n",
        "    Command-line arguments to the system related to your model.  Feel free to extend here.  \n",
        "    \"\"\"\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    parser.add_argument('--device', type=str, default=device, help='set device')\n",
        "\n",
        "    # Some common arguments for your convenience\n",
        "    parser.add_argument('--seed', type=int, default=0, help='RNG seed (default = 0)')\n",
        "    parser.add_argument('--epochs', type=int, default=10, help='num epochs to train for')\n",
        "    parser.add_argument('--lr', type=float, default=1e-3)\n",
        "    parser.add_argument('--batch_size', type=int, default=2, help='batch size')\n",
        "\n",
        "    # 65 is all you need for GeoQuery\n",
        "    parser.add_argument('--decoder_len_limit', type=int, default=65, help='output length limit of the decoder')\n",
        "\n",
        "    ### BEGIN OF FILL-IN ###\n",
        "    # Feel free to add other hyperparameters for your input dimension, etc. to control your network\n",
        "    # 50-200 might be a good range to start with for embedding and LSTM sizes\n",
        "    parser.add_argument('--reverse_input', action='store_true', help='reverse the input sequence')\n",
        "    parser.add_argument('--emb_dim', type=int, default=100, help='size of word embeddings')\n",
        "    parser.add_argument('--hidden_size', type=int, default=200, help='size of LSTM hidden states')\n",
        "    parser.add_argument('--attention', action='store_true', help='use attention mechanism')\n",
        "    parser.add_argument('--attention_type', type=str, default='additive', choices=['additive', 'multiplicative'], help='type of attention mechanism')\n",
        "    parser.add_argument('--output_max_len', type=int, default=65, help='maximum length of the output sequence')\n",
        "    parser.add_argument('--rnn_type', type=str, default='lstm', choices=['lstm', 'gru'], help='type of RNN to use')\n",
        "    parser.add_argument('--dropout', type=float, default=0.2, help='dropout probability')\n",
        "\n",
        "    ### END OF FILL-IN ###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSGZ9cNioNVN"
      },
      "source": [
        "# Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwTvXbgnn-I2"
      },
      "source": [
        "## Nearest Neighbor Baseline\n",
        "\n",
        "This is a baseline implementation of a nearest neighbor model that we provided for you. It returns the query of the training string that has the highest token overlap with a test string. We also provided you some code for you to run this model. You can run the following sections here and see how well it does to verify your setup and get some insights of how to run and evaluate your own model later on. It should get about 24/120 denotation accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cow7Qq77tp76"
      },
      "source": [
        "##Nearest Neighbor Model Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hdGc2IoIn7JO"
      },
      "outputs": [],
      "source": [
        "class NearestNeighborSemanticParser(object):\n",
        "    \"\"\"\n",
        "    Semantic parser that uses Jaccard similarity to find the most similar input example to a particular question and\n",
        "    returns the associated logical form.\n",
        "    \"\"\"\n",
        "    def __init__(self, training_data: List[Example]):\n",
        "        self.training_data = training_data\n",
        "\n",
        "    def decode(self, test_data: List[Example]) -> List[List[Derivation]]:\n",
        "        \"\"\"\n",
        "        :param test_data: List[Example] to decode\n",
        "        :return: A list of k-best lists of Derivations. A Derivation consists of the underlying Example, a probability,\n",
        "        and a tokenized input string. If you're just doing one-best decoding of example ex and you\n",
        "        produce output y_tok, you can just return the k-best list [Derivation(ex, 1.0, y_tok)]\n",
        "        \"\"\"\n",
        "        test_derivs = []\n",
        "        for test_ex in test_data:\n",
        "            test_words = test_ex.x_tok\n",
        "            best_jaccard = -1\n",
        "            best_train_ex = None\n",
        "            # Find the highest word overlap with the train data\n",
        "            for train_ex in self.training_data:\n",
        "                # Compute word overlap with Jaccard similarity\n",
        "                train_words = train_ex.x_tok\n",
        "                overlap = len(frozenset(train_words) & frozenset(test_words))\n",
        "                jaccard = overlap/float(len(frozenset(train_words) | frozenset(test_words)))\n",
        "                if jaccard > best_jaccard:\n",
        "                    best_jaccard = jaccard\n",
        "                    best_train_ex = train_ex\n",
        "            # Note that this is a list of a single Derivation\n",
        "            test_derivs.append([Derivation(test_ex, 1.0, best_train_ex.y_tok)])\n",
        "        return test_derivs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6zt9q0Ct9LS"
      },
      "source": [
        "##Nearest Neighbor Runner Code\n",
        "\n",
        "The following section includes code that help you run the nearest neighbor model. This could be a good template for you to run your model as well. Notice that here the _parse_args() function calls the add_models_args() above. So you can extend the add_models_args() function as before to include any arguments you may want. Another thing to notice here is that we set the argument do_nearest_neighbor=True to run this nearest neighbor experiment, and you may want to set it to False to run your own model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3hkC7TBur1aw"
      },
      "outputs": [],
      "source": [
        "def _parse_args():\n",
        "    \"\"\"\n",
        "    Command-line arguments to the system. --model switches between the main modes you'll need to use. The other arguments\n",
        "    are provided for convenience.\n",
        "    :return: the parsed args bundle\n",
        "    \"\"\"\n",
        "    parser = argparse.ArgumentParser(description='main.py')\n",
        "    \n",
        "    # General system running and configuration options\n",
        "    parser.add_argument('--do_nearest_neighbor', dest='do_nearest_neighbor', default=False, action='store_true', help='run the nearest neighbor model')\n",
        "\n",
        "    parser.add_argument('--train_path', type=str, default='data/geo_train.tsv', help='path to train data')\n",
        "    parser.add_argument('--dev_path', type=str, default='data/geo_dev.tsv', help='path to dev data')\n",
        "    parser.add_argument('--test_path', type=str, default='data/geo_test.tsv', help='path to blind test data')\n",
        "    parser.add_argument('--test_output_path', type=str, default='test_output.tsv', help='path to write blind test results')\n",
        "    parser.add_argument('--domain', type=str, default='geo', help='domain (geo for geoquery)')\n",
        "    parser.add_argument('--no_java_eval', dest='perform_java_eval', default=True, action='store_false', help='run evaluation of constructed query against java backend')\n",
        "    parser.add_argument('--print_dataset', dest='print_dataset', default=False, action='store_true', help=\"Print some sample data on loading\")\n",
        "    add_models_args(parser) \n",
        "\n",
        "    args = parser.parse_args(\"\")\n",
        "    return args\n",
        "\n",
        "def main(args):\n",
        "    # Load the training and test data \n",
        "    train, dev, test = load_datasets(args.train_path, args.dev_path, args.test_path, domain=args.domain)\n",
        "    train_data_indexed, dev_data_indexed, test_data_indexed, input_indexer, output_indexer = index_datasets(train, dev, test, args.decoder_len_limit)\n",
        "    print(\"%i train exs, %i dev exs, %i input types, %i output types\" % (len(train_data_indexed), len(dev_data_indexed), len(input_indexer), len(output_indexer)))\n",
        "    if args.print_dataset:\n",
        "        print(\"Input indexer: %s\" % input_indexer)\n",
        "        print(\"Output indexer: %s\" % output_indexer)\n",
        "        print(\"Here are some examples post tokenization and indexing:\")\n",
        "        for i in range(0, min(len(train_data_indexed), 10)):\n",
        "            print(train_data_indexed[i])\n",
        "    if args.do_nearest_neighbor:\n",
        "        decoder = NearestNeighborSemanticParser(train_data_indexed)\n",
        "    else:\n",
        "        decoder = train_model_encdec(train_data_indexed, dev_data_indexed, input_indexer, output_indexer, args)\n",
        "    print(\"=======DEV SET=======\")\n",
        "    evaluate(dev_data_indexed, decoder, use_java=args.perform_java_eval)\n",
        "    print(\"=======FINAL PRINTING ON BLIND TEST=======\")\n",
        "    evaluate(test_data_indexed, decoder, print_output=False, outfile=args.test_output_path, use_java=args.perform_java_eval)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8La9V_oSURuq",
        "outputId": "663f37e0-2464-401e-c87c-c46cb6664160"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 480 exs from file data/geo_train.tsv\n",
            "Loaded 120 exs from file data/geo_dev.tsv\n",
            "Loaded 280 exs from file data/geo_test.tsv\n",
            "480 train exs, 120 dev exs, 238 input types, 153 output types\n",
            "=======DEV SET=======\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading JAR files: evaluator.jar\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example 49\n",
            "  x      = \"what is the longest river flowing through new york ?\"\n",
            "  y_tok  = \"['_answer', '(', 'NV', ',', '_longest', '(', 'V0', ',', '(', '_river', '(', 'V0', ')', ',', '_traverse', '(', 'V0', ',', 'NV', ')', ',', '_const', '(', 'V0', ',', '_stateid', '(', \"'\", 'new', 'york', \"'\", ')', ')', ')', ')', ')']\"\n",
            "  y_pred = \"['_answer', '(', 'NV', ',', '_longest', '(', 'V0', ',', '(', '_river', '(', 'V0', ')', ',', '_loc', '(', 'V0', ',', 'NV', ')', ',', '_const', '(', 'V0', ',', '_stateid', '(', \"'\", 'new', 'york', \"'\", ')', ')', ')', ')', ')']\"\n",
            "Example 99\n",
            "  x      = \"what is the lowest point in california ?\"\n",
            "  y_tok  = \"['_answer', '(', 'NV', ',', '_lowest', '(', 'V0', ',', '(', '_place', '(', 'V0', ')', ',', '_loc', '(', 'V0', ',', 'NV', ')', ',', '_const', '(', 'V0', ',', '_stateid', '(', 'california', ')', ')', ')', ')', ')']\"\n",
            "  y_pred = \"['_answer', '(', 'NV', ',', '_lowest', '(', 'V0', ',', '(', '_place', '(', 'V0', ')', ',', '_loc', '(', 'V0', ',', 'NV', ')', ',', '_const', '(', 'V0', ',', '_stateid', '(', 'wisconsin', ')', ')', ')', ')', ')']\"\n",
            "Exact logical form matches: 15 / 120 = 0.125\n",
            "Token-level accuracy: 2722 / 3908 = 0.697\n",
            "Denotation matches: 24 / 120 = 0.200\n",
            "=======FINAL PRINTING ON BLIND TEST=======\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading JAR files: evaluator.jar\n"
          ]
        }
      ],
      "source": [
        "# Initalize arguments\n",
        "args = _parse_args()\n",
        "random.seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "\n",
        "# Set arguments here\n",
        "args.do_nearest_neighbor = True \n",
        "\n",
        "main(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR66_KQFoZiH"
      },
      "source": [
        "# Advanced Model(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIdP1tlOoBU9"
      },
      "source": [
        "## Seq2Seq Parser\n",
        "\n",
        "Now you need to implement your own Seq2Seq model. There are some sections that we provided you as well to simplify things, but you should generally feel free to implement an encoder-decoder model as you wish! Specific descriptions of what each part does in this notebook can be found in the handout. A useful pytorch and Seq2Seq tutorial can also be found here:\n",
        "[tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSegg6g0nCp0"
      },
      "source": [
        "## Embedding layer\n",
        "We provided you this implementation of embedding layer using nn.module. It is very similar to what we asked you to implement in HW3 that maps words to embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cczHBOEam7eh"
      },
      "outputs": [],
      "source": [
        "class EmbeddingLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Embedding layer that has a lookup table of symbols that is [full_dict_size x input_dim]. Includes dropout.\n",
        "    Works for both non-batched and batched inputs\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, full_dict_size: int, embedding_dropout_rate: float):\n",
        "        \"\"\"\n",
        "        :param input_dim: dimensionality of the word vectors\n",
        "        :param full_dict_size: number of words in the vocabulary\n",
        "        :param embedding_dropout_rate: dropout rate to apply\n",
        "        \"\"\"\n",
        "        super(EmbeddingLayer, self).__init__()\n",
        "        self.dropout = nn.Dropout(embedding_dropout_rate)\n",
        "        self.word_embedding = nn.Embedding(full_dict_size, input_dim)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        :param input: either a non-batched input [sent len x voc size] or a batched input\n",
        "        [batch size x sent len x voc size]\n",
        "        :return: embedded form of the input words (last coordinate replaced by input_dim)\n",
        "        \"\"\"\n",
        "        embedded_words = self.word_embedding(input)\n",
        "        final_embeddings = self.dropout(embedded_words)\n",
        "        return final_embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y-3DlNDp1fS"
      },
      "source": [
        "## Encoder\n",
        "This is a RNN encoder model. This is a vetted implementation of an LSTM that is provided for your convenience; however, if you want to use a different encoder or build something yourself, you should feel free! Note that this implementation does not use GloVe embeddings, but you're free to use them if you want; you just need to modify the embedding layer. Note that updating embeddings during training is very important for good performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7Ivc-nOdm9y_"
      },
      "outputs": [],
      "source": [
        "class RNNEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    One-layer RNN encoder for batched inputs -- handles multiple sentences at once. To use in non-batched mode, call it\n",
        "    with a leading dimension of 1 (i.e., use batch size 1)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_emb_dim: int, hidden_size: int, bidirect: bool):\n",
        "        \"\"\"\n",
        "        :param input_emb_dim: size of word embeddings output by embedding layer\n",
        "        :param hidden_size: hidden size for the LSTM\n",
        "        :param bidirect: True if bidirectional, false otherwise\n",
        "        \"\"\"\n",
        "        super(RNNEncoder, self).__init__()\n",
        "        self.bidirect = bidirect\n",
        "        self.hidden_size = hidden_size\n",
        "        self.reduce_h_W = nn.Linear(hidden_size * 2, hidden_size, bias=True)\n",
        "        self.reduce_c_W = nn.Linear(hidden_size * 2, hidden_size, bias=True)\n",
        "        self.rnn = nn.LSTM(input_emb_dim, hidden_size, num_layers=1, batch_first=True,\n",
        "                               dropout=0., bidirectional=self.bidirect)\n",
        "\n",
        "    def get_output_size(self):\n",
        "        return self.hidden_size * 2 if self.bidirect else self.hidden_size\n",
        "\n",
        "    def sent_lens_to_mask(self, lens, max_length):\n",
        "        \"\"\"\n",
        "        Converts sentence lengths to a mask tensor.\n",
        "\n",
        "        This function creates a binary mask for a batch of sequences, where 1 represents\n",
        "        actual sequence elements and 0 represents padding.\n",
        "\n",
        "        Args:\n",
        "            lens (torch.Tensor): A 1D tensor containing the actual lengths of each sequence in the batch.\n",
        "            max_length (int): The maximum length to which all sequences are padded.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A 2D binary mask tensor of shape (batch_size, max_length).\n",
        "                        1 indicates actual sequence elements, 0 indicates padding.\n",
        "\n",
        "        Example:\n",
        "            If lens = [2, 4, 3] and max_length = 5, the output will be:\n",
        "            [[1, 1, 0, 0, 0],\n",
        "            [1, 1, 1, 1, 0],\n",
        "            [1, 1, 1, 0, 0]]\n",
        "        \"\"\"\n",
        "        return torch.from_numpy(np.asarray([[1 if j < lens.data[i].item() else 0 for j in range(0, max_length)] for i in range(0, lens.shape[0])]))\n",
        "\n",
        "    def forward(self, embedded_words, input_lens):\n",
        "        \"\"\"\n",
        "        Runs the forward pass of the LSTM\n",
        "        :param embedded_words: [batch size x sent len x input dim] tensor\n",
        "        :param input_lens: [batch size]-length vector containing the length of each input sentence\n",
        "        :return: output (each word's representation), context_mask (a mask of 0s and 1s\n",
        "        reflecting where the model's output should be considered), and h_t, a *tuple* containing\n",
        "        the final states h and c from the encoder for each sentence.\n",
        "        Note that output is only needed for attention, and context_mask is only used for batched attention.\n",
        "        \"\"\"\n",
        "        # Takes the embedded sentences, \"packs\" them into an efficient Pytorch-internal representation\n",
        "        packed_embedding = nn.utils.rnn.pack_padded_sequence(embedded_words, input_lens.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        # Runs the RNN over each sequence. Returns output at each position as well as the last vectors of the RNN\n",
        "        # state for each sentence (first/last vectors for bidirectional)\n",
        "        output, hn = self.rnn(packed_embedding)\n",
        "        # Unpacks the Pytorch representation into normal tensors\n",
        "        output, sent_lens = nn.utils.rnn.pad_packed_sequence(output)\n",
        "        max_length = max(input_lens.data).item()\n",
        "        context_mask = self.sent_lens_to_mask(sent_lens, max_length)\n",
        "\n",
        "        if self.bidirect:\n",
        "            h, c = hn[0], hn[1]\n",
        "            # Grab the representations from forward and backward LSTMs\n",
        "            h_, c_ = torch.cat((h[0], h[1]), dim=1), torch.cat((c[0], c[1]), dim=1)\n",
        "            # Reduce them by multiplying by a weight matrix so that the hidden size sent to the decoder is the same\n",
        "            # as the hidden size in the encoder\n",
        "            new_h = self.reduce_h_W(h_)\n",
        "            new_c = self.reduce_c_W(c_)\n",
        "            h_t = (new_h, new_c)\n",
        "        else:\n",
        "            h, c = hn[0][0], hn[1][0]\n",
        "            h_t = (h, c)\n",
        "        return (output, context_mask, h_t)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLj5wm_fqScb"
      },
      "source": [
        "## Decoder\n",
        "\n",
        "This is the decoder part of your encoder-decoder model that you need to finish implementing. Recall that at each step of decoding, a basic decoder is given an embedded input and last hidden states from the encoder. \n",
        "\n",
        "Here, the *embedded_input* parameter of the forward() function is the embedded input. The parameter *state* is a tuple of (h,c) from the encoder, representing the last hidden states and the context vector. *enc_output* is the encoder outputs of each word that you may need for attention, and *enc_context_mask* is the parameter you may need if you want to achieve batch attention.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4DQ939hhm2-c"
      },
      "outputs": [],
      "source": [
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, encoder_output_size, output_dict_size, attention=False, attention_type='additive', rnn_type='lstm'):\n",
        "        \n",
        "        super(RNNDecoder, self).__init__()\n",
        "        self.n_layers = 1\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.encoder_output_size = encoder_output_size\n",
        "        self.attention = attention\n",
        "        self.attention_type = attention_type\n",
        "        self.rnn_type = rnn_type\n",
        "        \n",
        "        ### BEGIN OF FILL-IN ###\n",
        "        \n",
        "        if rnn_type == 'lstm':\n",
        "            self.rnn = nn.LSTM(input_size, hidden_size, num_layers=self.n_layers, batch_first=True)\n",
        "        elif rnn_type == 'gru':\n",
        "            self.rnn = nn.GRU(input_size, hidden_size, num_layers=self.n_layers, batch_first=True)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported RNN type\")\n",
        "\n",
        "        #TODO: implement attention\n",
        " \n",
        "        self.out = nn.Linear(hidden_size, output_dict_size) \n",
        "\n",
        "        ### END OF FILL-IN ###\n",
        "\n",
        "    def forward(self, embedded_input, state, enc_output, enc_context_mask):\n",
        "        ### BEGIN OF FILL-IN ###\n",
        "\n",
        "        if self.rnn_type == 'lstm':\n",
        "            output, hidden = self.rnn(embedded_input, state)\n",
        "        else:  # GRU\n",
        "            raise Exception(\"implement GRU\")\n",
        "            output, hidden = self.rnn(embedded, state[0])\n",
        "        \n",
        "        # Generate output distribution\n",
        "        output = self.out(output)\n",
        "        log_probs = F.log_softmax(output, dim=-1)\n",
        "\n",
        "\n",
        "        ### End OF FILL-IN ###\n",
        "        \n",
        "        return log_probs, hidden\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9EGxPSVPDYc"
      },
      "source": [
        "## Helper functions\n",
        "Here are some helper functions that you may find useful in transforming your input and output data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "i1Jq2t6toVsm"
      },
      "outputs": [],
      "source": [
        "def make_padded_input_tensor(exs: List[Example], input_indexer: Indexer, max_len: int, reverse_input=False) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Takes the given Examples and their input indexer and turns them into a numpy array by padding them out to max_len.\n",
        "    Optionally reverses them.\n",
        "    :param exs: examples to tensor-ify\n",
        "    :param input_indexer: Indexer over input symbols; needed to get the index of the pad symbol\n",
        "    :param max_len: max input len to use (pad/truncate to this length)\n",
        "    :param reverse_input: True if we should reverse the inputs (useful if doing a unidirectional LSTM encoder)\n",
        "    :return: A [num example, max_len]-size array of indices of the input tokens\n",
        "    \n",
        "    Example:\n",
        "    Let's say we have:\n",
        "    - exs = [Example1, Example2, Example3]\n",
        "    - Example1.x_indexed = [1, 2, 3, 4]\n",
        "    - Example2.x_indexed = [5, 6]\n",
        "    - Example3.x_indexed = [7, 8, 9]\n",
        "    - input_indexer.index_of(PAD_SYMBOL) = 0\n",
        "    - max_len = 5\n",
        "    - reverse_input = False\n",
        "\n",
        "    The function will return:\n",
        "    np.array([\n",
        "        [1, 2, 3, 4, 0],\n",
        "        [5, 6, 0, 0, 0],\n",
        "        [7, 8, 9, 0, 0]\n",
        "    ])\n",
        "    \"\"\"\n",
        "    if reverse_input:\n",
        "        return np.array(\n",
        "            [[ex.x_indexed[len(ex.x_indexed) - 1 - i] if i < len(ex.x_indexed) else input_indexer.index_of(PAD_SYMBOL)\n",
        "              for i in range(0, max_len)]\n",
        "             for ex in exs])\n",
        "    else:\n",
        "        return np.array([[ex.x_indexed[i] if i < len(ex.x_indexed) else input_indexer.index_of(PAD_SYMBOL)\n",
        "                          for i in range(0, max_len)]\n",
        "                         for ex in exs])\n",
        "\n",
        "def make_padded_output_tensor(exs, output_indexer, max_len):\n",
        "    \"\"\"\n",
        "    Similar to make_padded_input_tensor, but does it on the outputs without the option to reverse input\n",
        "    :param exs:\n",
        "    :param output_indexer:\n",
        "    :param max_len:\n",
        "    :return: A [num example, max_len]-size array of indices of the output tokens\n",
        "    \"\"\"\n",
        "    return np.array([[ex.y_indexed[i] if i < len(ex.y_indexed) else output_indexer.index_of(PAD_SYMBOL) for i in range(0, max_len)] for ex in exs])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFTLXilGnXiQ"
      },
      "source": [
        "## Seq2Seq Parser\n",
        "This is the Seq2Seq module that integrates your encoder and decoder to make a full run. In your forward() function, you should pass the input to your encoder first, then pass the output of your encoder as well as your model input to your decoder, and compute and return the loss.\n",
        "\n",
        "You also need to implement the decode() function that takes input of your test data and return the predicted outputs. **Note that you should not need to call this function in your forward(), but you still need to implement it so that you can evaluate the result.** The evaluate() function from data.py will implicitly call it and evaluate using it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2IlesFt8V3pf"
      },
      "outputs": [],
      "source": [
        "###################################################################################################################\n",
        "# Please fill in the code in all the specified spaces\n",
        "###################################################################################################################\n",
        "import math\n",
        "class Seq2SeqSemanticParser(nn.Module):\n",
        "    def __init__(self, input_indexer, output_indexer, emb_dim, hidden_size, embedding_dropout=0.2,\n",
        "                 bidirect=True, use_luong=True, attention=True, output_max_len=65, reverse_input=False):\n",
        "        # We've included some args for setting up the input embedding and encoder\n",
        "        # You'll need to add code for output embedding and decoder\n",
        "        super(Seq2SeqSemanticParser, self).__init__()\n",
        "        self.input_indexer = input_indexer\n",
        "        self.output_indexer = output_indexer\n",
        "        \n",
        "        self.input_emb = EmbeddingLayer(emb_dim, len(input_indexer), embedding_dropout)\n",
        "        self.encoder = RNNEncoder(input_emb_dim=emb_dim, hidden_size=hidden_size, bidirect=bidirect)\n",
        "\n",
        "        ### BEGIN OF FILL-IN ###\n",
        "        self.output_emb = EmbeddingLayer(emb_dim, len(output_indexer), embedding_dropout)\n",
        "        encoder_output_size = self.encoder.get_output_size()\n",
        "        self.decoder = RNNDecoder(input_size=emb_dim, hidden_size=hidden_size, \n",
        "                                    encoder_output_size=encoder_output_size, output_dict_size=len(output_indexer),\n",
        "                                    attention=attention)\n",
        "        self.output_max_len = output_max_len\n",
        "       \n",
        "        ### END OF FILL-IN ###\n",
        "    \n",
        "    def forward(self, x_tensor, inp_lens_tensor, y_tensor, out_lens_tensor, args):\n",
        "        \"\"\"\n",
        "        :param x_tensor/y_tensor: either a non-batched input/output [sent len] vector of indices or a batched input/output\n",
        "        [batch size x sent len]. y_tensor contains the gold sequence(s) used for training\n",
        "        :param inp_lens_tensor/out_lens_tensor: either a vector of input/output length [batch size] or a single integer.\n",
        "        lengths aren't needed if you don't batchify the training.\n",
        "        :return: loss of the batch\n",
        "        \"\"\"\n",
        "        ### BEGIN OF FILL-IN ###\n",
        "        batch_size, max_out_len = y_tensor.shape\n",
        "\n",
        "        # Encode input\n",
        "        x_embedded = self.input_emb(x_tensor)\n",
        "        enc_output_each_word, enc_context_mask, enc_final_states = self.encode_input(x_embedded, inp_lens_tensor)\n",
        "\n",
        "        # Prepare decoder input (add start token)\n",
        "        start_token = torch.full((batch_size, 1), SOS_SYMBOL, device=y_tensor.device)\n",
        "        y_input = torch.cat([start_token, y_tensor[:, :-1]], dim=1)  # Teacher forcing\n",
        "        y_embedded = self.output_emb(y_input)\n",
        "\n",
        "        # Initialize decoder state\n",
        "        dec_hidden = enc_final_states\n",
        "\n",
        "        # Decode\n",
        "        # batch size x seq length x output dict size\n",
        "        log_probs, _ = self.decoder.forward(y_embedded, enc_final_states, enc_output_each_word, enc_context_mask)\n",
        "        \n",
        "        # Reshape log_probs for NLLLoss becomes [(batch size x seq len) x output dict size]\n",
        "        log_probs_reshaped = log_probs.view(-1, log_probs.size(-1))\n",
        "        # y_tensor is [batch size x seq len] -> [(batch size x seq len) x 1]\n",
        "        y_tensor_reshaped = y_tensor.view(-1)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss_function = nn.NLLLoss(ignore_index=self.pad_token_idx)\n",
        "        loss = loss_function(log_probs_reshaped, y_tensor_reshaped)\n",
        "\n",
        "        return loss\n",
        "        \n",
        "        ### END OF FILL-IN ###\n",
        "\n",
        "    def decode(self, test_data: List[Example]) -> List[List[Derivation]]:\n",
        "\n",
        "        \"\"\"\n",
        "        :param test_data: input test data as List[Example]. You can find the definition of the class Example() in data.py\n",
        "        \n",
        "        :return: your predicted outputs as List[List[Derivation]]. You can find more descriptions of the class Derivation() in data.py\n",
        "        \"\"\"\n",
        "\n",
        "        ### BEGIN OF FILL-IN ###\n",
        "        all_derivations = []\n",
        "        beam_size = 5  # You can adjust this\n",
        "\n",
        "        for example in test_data:\n",
        "            x_tensor = torch.tensor([example.x_indexed])\n",
        "            inp_lens_tensor = torch.tensor([len(example.x_indexed)])\n",
        "            \n",
        "            enc_output_each_word, enc_context_mask, enc_final_states = self.encode_input(x_tensor, inp_lens_tensor)\n",
        "            \n",
        "            # Initialize beam with start symbol\n",
        "            init_seq = [SOS_SYMBOL]\n",
        "            beam = Beam(beam_size)\n",
        "            beam.add((init_seq, enc_final_states), 0)  # Initial log prob is 0\n",
        "            \n",
        "            completed_sequences = []\n",
        "            \n",
        "            for _ in range(self.output_max_len):\n",
        "                new_beam = Beam(beam_size)\n",
        "                \n",
        "                for seq, state in beam.get_elts():\n",
        "                    last_token_idx = self.output_indexer.index_of(seq[-1])\n",
        "                    last_token = torch.tensor([[last_token_idx]])\n",
        "                    decoder_input_embedded = self.output_emb.forward(last_token)\n",
        "                    log_probs, new_state = self.decoder.forward(decoder_input_embedded, state, enc_output_each_word, enc_context_mask)\n",
        "\n",
        "                    log_probs = log_probs.squeeze(0) \n",
        "                    top_log_probs, top_indices = log_probs.topk(beam_size)\n",
        "                \n",
        "                    for log_prob, idx in zip(top_log_probs, top_indices):\n",
        "                        token = self.output_indexer.get_object(idx.item())\n",
        "                        new_seq = seq + [token]\n",
        "                        new_score = beam.scores[beam.elts.index((seq, state))] + log_prob.item()\n",
        "                        new_beam.add((new_seq, new_state), new_score)\n",
        "                        \n",
        "                        if token == EOS_SYMBOL:\n",
        "                            completed_sequences.append((new_seq, new_score))\n",
        "                \n",
        "                beam = new_beam\n",
        "                \n",
        "                # Early stopping if beam is empty (all sequences ended with EOS)\n",
        "                if len(beam) == 0:\n",
        "                    break\n",
        "        \n",
        "        for seq, state in beam.get_elts():\n",
        "            score = beam.scores[beam.elts.index((seq, state))]\n",
        "            completed_sequences.append((seq, score))\n",
        "        \n",
        "        all_sequences = sorted(completed_sequences + list(beam.get_elts_and_scores()), key=lambda x: x[1], reverse=True)\n",
        "        top_sequences = all_sequences[:beam_size]\n",
        "\n",
        "        example_derivations = []\n",
        "        for seq, score in top_sequences:\n",
        "            output_tokens = [token for token in seq[1:] if token not in {SOS_SYMBOL, EOS_SYMBOL}]\n",
        "            derivation = Derivation(example, score, output_tokens)\n",
        "            example_derivations.append(derivation)\n",
        "        \n",
        "        all_derivations.append(example_derivations)\n",
        "\n",
        "        return all_derivations\n",
        "\n",
        "        \n",
        "        ### END OF FILL-IN ###\n",
        "\n",
        "\n",
        "    def encode_input(self, x_tensor, inp_lens_tensor):\n",
        "        \"\"\"\n",
        "        Runs the encoder (input embedding layer and encoder as two separate modules) on a tensor of inputs x_tensor with\n",
        "        inp_lens_tensor lengths.\n",
        "        YOU DO NOT HAVE TO USE THIS FUNCTION. It's meant to illustrate the usage of EmbeddingLayer and RNNEncoder\n",
        "        as they're given to you, as well as show what kinds of inputs/outputs you need from your encoding phase.\n",
        "        :param x_tensor: [batch size, sent len] tensor of input token indices\n",
        "        :param inp_lens_tensor: [batch size] vector containing the length of each sentence in the batch\n",
        "        :param model_input_emb: EmbeddingLayer\n",
        "        :param model_enc: RNNEncoder\n",
        "        :return: the encoder outputs (per word), the encoder context mask (matrix of 1s and 0s reflecting which words\n",
        "        are real and which ones are pad tokens), and the encoder final states (h and c tuple). ONLY THE ENCODER FINAL\n",
        "        STATES are needed for the basic seq2seq model. enc_output_each_word is needed for attention, and\n",
        "        enc_context_mask is needed to batch attention.\n",
        "\n",
        "        E.g., calling this with x_tensor (0 is pad token):\n",
        "        [[12, 25, 0],\n",
        "        [1, 2, 3],\n",
        "        [2, 0, 0]]\n",
        "        inp_lens = [2, 3, 1]\n",
        "        will return outputs with the following shape:\n",
        "        enc_output_each_word = 3 x 3 x dim, enc_context_mask = [[1, 1, 0], [1, 1, 1], [1, 0, 0]],\n",
        "        enc_final_states = 3 x dim\n",
        "        \"\"\"\n",
        "        input_emb = self.input_emb.forward(x_tensor)\n",
        "        (enc_output_each_word, enc_context_mask, enc_final_states) = self.encoder.forward(input_emb, inp_lens_tensor)\n",
        "        enc_final_states_reshaped = (enc_final_states[0].unsqueeze(0), enc_final_states[1].unsqueeze(0))\n",
        "        return (enc_output_each_word, enc_context_mask, enc_final_states_reshaped)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdhXZgH7PVuH"
      },
      "source": [
        "# Train\n",
        "\n",
        "This is your main training function. You may want to instantiate a Seq2SeqSemanticParser() model and an optimizer, train it over epochs, and evaluate on your dev_data after training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vKmbdi4xoTrS"
      },
      "outputs": [],
      "source": [
        "def train_model_encdec(train_data: List[Example], dev_data: List[Example], input_indexer, output_indexer, args) -> Seq2SeqSemanticParser:\n",
        "    \"\"\"\n",
        "    Function to train the encoder-decoder model on the given data.\n",
        "    :param train_data:\n",
        "    :param dev_data: Development set in case you wish to evaluate during training\n",
        "    :param input_indexer: Indexer of input symbols\n",
        "    :param output_indexer: Indexer of output symbols\n",
        "    :param args:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create indexed input\n",
        "    input_max_len = np.max(np.asarray([len(ex.x_indexed) for ex in train_data]))\n",
        "    all_train_input_data = make_padded_input_tensor(train_data, input_indexer, input_max_len, reverse_input=args.reverse_input)\n",
        "    all_test_input_data = make_padded_input_tensor(dev_data, input_indexer, input_max_len, reverse_input=args.reverse_input)\n",
        "\n",
        "    output_max_len = np.max(np.asarray([len(ex.y_indexed) for ex in train_data]))\n",
        "    all_train_output_data = make_padded_output_tensor(train_data, output_indexer, output_max_len)\n",
        "    all_test_output_data = make_padded_output_tensor(dev_data, output_indexer, output_max_len)\n",
        "\n",
        "    if args.print_dataset:\n",
        "        print(\"Train length: %i\" % input_max_len)\n",
        "        print(\"Train output length: %i\" % np.max(np.asarray([len(ex.y_indexed) for ex in train_data])))\n",
        "        print(\"Train matrix: %s; shape = %s\" % (all_train_input_data, all_train_input_data.shape))\n",
        "\n",
        "    # First create a model. Then loop over epochs, loop over examples, and given some indexed words\n",
        "    # call your seq-to-seq model, accumulate losses, update parameters, \n",
        "    # Some started code is provided below, but you are NOT necessary to follow them\n",
        "    \n",
        "    ### BEGIN OF FILL-IN ###\n",
        "    model = Seq2SeqSemanticParser(input_indexer, output_indexer, args.emb_dim, args.hidden_size)\n",
        "    model.to(args.device)\n",
        "\n",
        "    # Define optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "\n",
        "    for epoch in tqdm(range(args.epochs)):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        batch_order = list(range(0, int(all_train_input_data.shape[0] / args.batch_size)))\n",
        "        random.shuffle(batch_order)\n",
        "\n",
        "        for batch_idx in batch_order:\n",
        "            start_idx = batch_idx * args.batch_size\n",
        "            end_idx = (batch_idx + 1) * args.batch_size\n",
        "\n",
        "            x_tensor = torch.from_numpy(all_train_input_data[start_idx:end_idx]).to(args.device)\n",
        "            y_tensor = torch.from_numpy(all_train_output_data[start_idx:end_idx]).to(args.device)\n",
        "            inp_lens_tensor = torch.sum(x_tensor != 0, dim=1)\n",
        "            out_lens_tensor = torch.sum(y_tensor != 0, dim=1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss = model(x_tensor, inp_lens_tensor, y_tensor, out_lens_tensor, args)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(batch_order)\n",
        "        print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    \n",
        "    ### END OF FILL-IN ###\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-dKTctjRQfE"
      },
      "source": [
        "# Now run your code!\n",
        "\n",
        "Now that you finished implementing your model, you can run your code just as you did for the nearest neighbor baseline! Make sure to include any arguments you would like in the *add_models_args()* function and set *do_nearest_neighbor=False*. Note that it takes roughly 8 minutes if you are running on CPU and 2 minutes if you are running on GPU. Besides, don't forget to use the *evaluate()* function to generate and save outputs for your test data and submit them to gradescope! You can do it in your *main()* function as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "3ANVlhIkjD9_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 480 exs from file data/geo_train.tsv\n",
            "Loaded 120 exs from file data/geo_dev.tsv\n",
            "Loaded 280 exs from file data/geo_test.tsv\n",
            "480 train exs, 120 dev exs, 238 input types, 153 output types\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bcb61f4c4cb441368ce41e6a110d69d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "RuntimeError",
          "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[21], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Set arguments here\u001b[39;00m\n\u001b[1;32m      7\u001b[0m args\u001b[38;5;241m.\u001b[39mdo_nearest_neighbor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \n\u001b[0;32m----> 9\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[6], line 38\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     36\u001b[0m     decoder \u001b[38;5;241m=\u001b[39m NearestNeighborSemanticParser(train_data_indexed)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     decoder \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_encdec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_indexed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_data_indexed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=======DEV SET=======\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m evaluate(dev_data_indexed, decoder, use_java\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mperform_java_eval)\n",
            "Cell \u001b[0;32mIn[20], line 55\u001b[0m, in \u001b[0;36mtrain_model_encdec\u001b[0;34m(train_data, dev_data, input_indexer, output_indexer, args)\u001b[0m\n\u001b[1;32m     51\u001b[0m out_lens_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(y_tensor \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     53\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 55\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp_lens_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_lens_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     58\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
            "File \u001b[0;32m/cis530hw4/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/cis530hw4/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[19], line 40\u001b[0m, in \u001b[0;36mSeq2SeqSemanticParser.forward\u001b[0;34m(self, x_tensor, inp_lens_tensor, y_tensor, out_lens_tensor, args)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Encode input\u001b[39;00m\n\u001b[1;32m     39\u001b[0m x_embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_emb(x_tensor)\n\u001b[0;32m---> 40\u001b[0m enc_output_each_word, enc_context_mask, enc_final_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_embedded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp_lens_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Prepare decoder input (add start token)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m start_token \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((batch_size, \u001b[38;5;241m1\u001b[39m), SOS_SYMBOL, device\u001b[38;5;241m=\u001b[39my_tensor\u001b[38;5;241m.\u001b[39mdevice)\n",
            "Cell \u001b[0;32mIn[19], line 164\u001b[0m, in \u001b[0;36mSeq2SeqSemanticParser.encode_input\u001b[0;34m(self, x_tensor, inp_lens_tensor)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_tensor, inp_lens_tensor):\n\u001b[1;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m    Runs the encoder (input embedding layer and encoder as two separate modules) on a tensor of inputs x_tensor with\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m    inp_lens_tensor lengths.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    enc_final_states = 3 x dim\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m     input_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_emb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     (enc_output_each_word, enc_context_mask, enc_final_states) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mforward(input_emb, inp_lens_tensor)\n\u001b[1;32m    166\u001b[0m     enc_final_states_reshaped \u001b[38;5;241m=\u001b[39m (enc_final_states[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), enc_final_states[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n",
            "Cell \u001b[0;32mIn[8], line 22\u001b[0m, in \u001b[0;36mEmbeddingLayer.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m     17\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    :param input: either a non-batched input [sent len x voc size] or a batched input\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    [batch size x sent len x voc size]\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    :return: embedded form of the input words (last coordinate replaced by input_dim)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     embedded_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     final_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(embedded_words)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m final_embeddings\n",
            "File \u001b[0;32m/cis530hw4/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/cis530hw4/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/cis530hw4/.venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/cis530hw4/.venv/lib/python3.10/site-packages/torch/nn/functional.py:2264\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2258\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2259\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2260\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
          ]
        }
      ],
      "source": [
        "# Initalize arguments\n",
        "args = _parse_args()\n",
        "random.seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "\n",
        "# Set arguments here\n",
        "args.do_nearest_neighbor = False \n",
        "\n",
        "main(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82oY2FwURdR-"
      },
      "source": [
        "# Submission\n",
        "\n",
        "These are the files you have to submit to gradescope\n",
        "\n",
        "1. Report\n",
        "2. The prediction output of basic encoder-decoder and attention model as \"test_output.tsv\" (from the Files section of sidebar)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
